{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy example\n",
    "from bpe import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "text = \"aaabdaaabac\"\n",
    "tokenizer.train(text, 256 + 3) # 256 are the byte tokens, then do 3 merges\n",
    "print(tokenizer.encode(text))\n",
    "# [258, 100, 258, 97, 99]\n",
    "print(tokenizer.decode([258, 100, 258, 97, 99]))\n",
    "print(text == tokenizer.decode(tokenizer.encode(text)))\n",
    "# aaabdaaabac\n",
    "# tokenizer.save(\"toy\")\n",
    "# writes two files: toy.model (for loading) and toy.vocab (for viewing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode and decode match: True\n"
     ]
    }
   ],
   "source": [
    "# 验证Tokenizer在encoder再decode之后与原文一致\n",
    "from bpe import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.load(\"manual.model\")\n",
    "with open('./manual.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "print(f\"encode and decode match: {text == tokenizer.decode(tokenizer.encode(text))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 tokens of text1 [11610, 3898, 355, 262, 11773, 2059, 286, 350, 18754, 287]\n",
      "length: 185\n",
      "first 10 tokens of text2 [39355, 248, 18803, 27764, 99, 19526, 235, 164, 106, 118]\n",
      "length: 306\n",
      "博士\n",
      "学位论文\n"
     ]
    }
   ],
   "source": [
    "# 使用gpt2 tokenizer encode 示例句子\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "# 加载 GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 测试 tokenizer\n",
    "text1 = \"Originated as the Imperial University of Peking in 1898, Peking University was China’s first national comprehensive university and the supreme education authority at the time. Since the founding of the People’s Republic of China in 1949, it has developed into a comprehensive university with fundamental education and research in both humanities and science. The reform and opening-up of China in 1978 has ushered in a new era for the University unseen in history. And its merger with Beijing Medical University in 2000 has geared itself up for all-round and vibrant growth in such fields as science, engineering, medicine, agriculture, humanities and social sciences. Supported by the “211 Project” and the “985 Project”, the University has made remarkable achievements, such as optimizing disciplines, cultivating talents, recruiting high-caliber teachers, as well as teaching and scientific research, which paves the way for a world-class university.\"\n",
    "encoded_input = tokenizer(text1)['input_ids']\n",
    "print(\"first 10 tokens of text1\", encoded_input[:10])\n",
    "print(\"length:\", len(encoded_input))\n",
    "text2 =\"博士学位论文应当表明作者具有独立从事科学研究工作的能力，并在科学或专门技术上做出创造性的成果。博士学位论文或摘要，应当在答辩前三个月印送有关单位，并经同行评议。学位授予单位应当聘请两位与论文有关学科的专家评阅论文，其中一位应当是外单位的专家。评阅人应当对论文写详细的学术评语，供论文答辩委员会参考。\"\n",
    "encoded_input = tokenizer(text2)['input_ids']\n",
    "print(\"first 10 tokens of text2\", encoded_input[:10])\n",
    "print(\"length:\", len(encoded_input))\n",
    "print(tokenizer.decode([39355, 248, 18803]))\n",
    "print(tokenizer.decode([27764, 99, 19526, 235, 164, 106, 118, 23877, 229]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 Bytes of utf-8: [79, 114, 105, 103, 105, 110, 97, 116, 101, 100]\n",
      "length: 965\n",
      "first 10 tokens of text1 [79, 114, 105, 103, 105, 110, 97, 116, 101, 100]\n",
      "length: 947\n",
      "\n",
      "first 10 Bytes of utf-8: [229, 141, 154, 229, 163, 171, 229, 173, 166, 228]\n",
      "length: 447\n",
      "first 10 tokens of text2 [457, 512, 524, 711, 642, 341, 456, 675, 353, 231]\n",
      "length: 119\n",
      "博士\n",
      "学位论文\n"
     ]
    }
   ],
   "source": [
    "# 使用我训练的 tokenizer encode 示例句子\n",
    "from bpe import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.load(\"manual.model\")\n",
    "text1 = \"Originated as the Imperial University of Peking in 1898, Peking University was China’s first national comprehensive university and the supreme education authority at the time. Since the founding of the People’s Republic of China in 1949, it has developed into a comprehensive university with fundamental education and research in both humanities and science. The reform and opening-up of China in 1978 has ushered in a new era for the University unseen in history. And its merger with Beijing Medical University in 2000 has geared itself up for all-round and vibrant growth in such fields as science, engineering, medicine, agriculture, humanities and social sciences. Supported by the “211 Project” and the “985 Project”, the University has made remarkable achievements, such as optimizing disciplines, cultivating talents, recruiting high-caliber teachers, as well as teaching and scientific research, which paves the way for a world-class university.\"\n",
    "encoded_text1 = tokenizer.encode(text1)\n",
    "utf_encoded_text1 = [byte for byte in text1.encode('utf-8')]\n",
    "print(\"first 10 Bytes of utf-8:\", utf_encoded_text1[:10])\n",
    "print(\"length:\", len(utf_encoded_text1))\n",
    "print(\"first 10 tokens of text1\", encoded_text1[:10])\n",
    "print(\"length:\", len(encoded_text1))\n",
    "\n",
    "print(\"\")\n",
    "text2 =\"博士学位论文应当表明作者具有独立从事科学研究工作的能力，并在科学或专门技术上做出创造性的成果。博士学位论文或摘要，应当在答辩前三个月印送有关单位，并经同行评议。学位授予单位应当聘请两位与论文有关学科的专家评阅论文，其中一位应当是外单位的专家。评阅人应当对论文写详细的学术评语，供论文答辩委员会参考。\"\n",
    "encoded_text2 = tokenizer.encode(text2)\n",
    "utf_encoded_text2 = [byte for byte in text2.encode('utf-8')]\n",
    "print(\"first 10 Bytes of utf-8:\", utf_encoded_text2[:10])\n",
    "print(\"length:\", len(utf_encoded_text2))\n",
    "print(\"first 10 tokens of text2\", encoded_text2[:10])\n",
    "print(\"length:\", len(encoded_text2))\n",
    "print(tokenizer.decode([457]))\n",
    "print(tokenizer.decode([512]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简要解释长度上和具体token上不同的原因是什么? \n",
    "\n",
    "因为使用的训练语料不同。所以在合并的时候，byte character合并的顺序和程度不同。\\\n",
    "text1英文文本，可以看到因为我们的tokenizer的训练语料基本没有英文，所以encode出来基本与unicode编码的byte character差不多，而gpt2的tokenizer编码得到的token sequence则更短，表示其对英语的合并比我们的tokenizer更高效。\\\n",
    "text2中文文本，可以看到gpt2 tokenizer用三个token表示“博士”，九个token表示“学位论文”，而我们的tokenizer用一个token表示“博士”，一个token表示“学位论文”。这是因为训练我们的tokenizer使用的语料与text2高度相关，其中很多词语在manual.txt中出现频率较高，进而被合并。而在gpt2的训练语料中中文语料相对较少，text2中相关的词语在其中的频率相对更不容易被合并，所以gpt2 tokenizer在text2上的编码效率更低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练tokenizer\n",
    "from bpe import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "with open('./manual.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "tokenizer.train(text, 1024)\n",
    "print(tokenizer.encode(text))\n",
    "print(text == tokenizer.decode(tokenizer.encode(text)))\n",
    "tokenizer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
